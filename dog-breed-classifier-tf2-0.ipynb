{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Initial Setup\n\nLet's start by importing some packages."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\n\nfrom tqdm import tqdm\nfrom os import listdir, makedirs\nfrom os.path import join","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Parameters\n\nHere we define the hyperparameters for training our model. We recommend keeping the defaults for your first run ðŸ™‚"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Hyperparams if GPU is available\nif tf.test.is_gpu_available():\n    # GPU\n    BATCH_SIZE = 16  # Number of images used in each iteration\n    EPOCHS = 3  # Number of passes through entire dataset\n    \n# Hyperparams for CPU training\nelse:\n    # CPU\n    BATCH_SIZE = 4\n    EPOCHS = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data\n\nThe dog-breed dataset is already attached to your workspace. It should be available at `/floyd/input/dogbreed`. (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n\n*Note:* We set the `NUM_CLASSES` variable to 10 to keep only the Top 10 dog breeds (so you can train the model on a CPU machine in a reasonable time). Feel free to increase this if you're trying this on a GPU!"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Keep only the first 10 classes\nNUM_CLASSES = 10 \n\ndata_dir = '/floyd/input/dogbreed/' # ADD path/to/dataset\nlabels = pd.read_csv(join(data_dir, 'labels.csv')) # EDIT WITH YOUR LABELS FILE NAME\nprint(\"Total number of images in the dataset: {}\".format(len(listdir(join(data_dir, 'train'))))) \n\nprint(\"Top {} labels (sorted by number of samples)\".format(NUM_CLASSES))\n(labels\n .groupby(\"breed\")\n .count()\n .sort_values(\"id\", ascending=False)\n .head(NUM_CLASSES)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transfer learning and data augmentation\n\nThis is the list of the top labels(dog breeds) sorted by the number of available samples in the dataset. If you notice, we only have ~100 examples of each breed - not much to train a deep learning model from scratch. \n\nSince we have only a few samples per label, we will\n- Set up a Transfer Learning task using the Xception model (pre-trained on ImageNet)\n- Use [data augmentation](https://en.wikipedia.org/wiki/Convolutional_neural_network#Artificial_data) to artificially increase the number of training images. We do this by manipulating (scaling, zooming, rotating, etc.) the input images.\n\nHere is an example of what data augmentation does"},{"metadata":{"trusted":false},"cell_type":"code","source":"from support import data_augmentation_example\n\ninput_path = '/floyd/input/dogbreed/train/00a338a92e4e7bf543340dc849230e75.jpg' # An example image from the dataset\ncount = 9 # Number of samples to show\n\ndata_augmentation_example(input_path, count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train - Val Split\n\nWe split the dataset into two sets, one for training and one for validation. Splitting the dataset is a common step when the validation and/or test sets are not provided."},{"metadata":{"trusted":false},"cell_type":"code","source":"from support import split_train_val\n\n# Make sure to have the same split\nSEED = 2018\n\n(train_idx, valid_idx, ytr, yv, labels, selected_breed_list) = split_train_val(labels, NUM_CLASSES, seed=SEED)\n\nfrom keras.preprocessing import image\nfrom support import show_images\n\nshow_images(NUM_CLASSES, labels, data_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n\nBefore feeding the data into the model, we have to preprocess the images.\n\n- The `xception.preprocess_input` method will normalize the value of each pixel to a number between -1 and 1, sample-wise.\n- Data augmentation: The `ImageDataGenerator` class will apply the distortions (defined by the parameters) *during* the training process. \n\n**Note:**: Be careful with data augmentation - the agumentation step is done during the training process and it could [become a bottleneck in the input data pipeline](https://www.tensorflow.org/performance/datasets_performance) if the CPU is not fast enough to provide the preprocessed/augmented data for GPU computation."},{"metadata":{"trusted":false},"cell_type":"code","source":"# DATA LOADER\nfrom keras.applications import xception\nfrom support import read_img\n\nINPUT_SIZE = 299 # width/height of image in pixels (as expected by Xception model)\n\nx_train = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, img_id in tqdm(enumerate(labels['id'])):\n    img = read_img(img_id, data_dir, 'train', (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_train[i] = x\nprint('\\nTotal Images shape: {}'.format(x_train.shape))\n\nXtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint('Train (images, H, W, C):', Xtr.shape,\n      '\\nVal (images, H, W, C):', Xv.shape, \n      '\\n\\nTrain samples (images, labels)', ytr.shape,\n      '\\nValidation samples (images, labels)', yv.shape)\n\n# Data Loader\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rotation_range=45,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.25,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model definition\n\nTo speed things up, we will apply Transfer Learning by building new layers on top of the pre-trained [Xception](https://arxiv.org/abs/1610.02357) model. \n\n<img src=\"https://raw.githubusercontent.com/floydhub/image-classification-template/master/images/xception.png\" width=\"600\" height=\"600\" align=\"center\"/>\n\n*Image from the [paper](https://arxiv.org/abs/1610.02357)*"},{"metadata":{"trusted":false},"cell_type":"code","source":"### MODEL - BOTTLENECK FEATURES - OPTMIZER\n\nfrom keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.models import Model, Input\n\n# Download and create the pre-trained Xception model for transfer learning\nbase_model = xception.Xception(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = BatchNormalization()(x)\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dropout(0.5)(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)\n# and a logistic layer -- let's say we have NUM_CLASSES classes\npredictions = Dense(NUM_CLASSES, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional Xception layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\noptimizer = RMSprop(lr=0.001, rho=0.9)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train CNN model\n\nIf you left the default hyperpameters in the Notebook untouched, your training should take approximately:\n\n- On CPU machine: 15-20 minutes for 1 epoch.\n- On GPU machine: 1 minute for 3 epochs (*soo much faster*).\n\nYou should get an accuracy of > 96% on the validation set. *Note*: The model will start overfitting after 2 to 3 epochs, since we're already starting from a pre-trained Xception model."},{"metadata":{"trusted":false},"cell_type":"code","source":"# TRAINING\nhist = model.fit_generator(train_datagen.flow(Xtr, ytr, batch_size=BATCH_SIZE),\n                           steps_per_epoch=train_idx.sum() // BATCH_SIZE,\n                           epochs=EPOCHS,\n                           validation_data=test_datagen.flow(Xv, yv, batch_size=BATCH_SIZE),\n                           validation_steps=valid_idx.sum() // BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate on validation set"},{"metadata":{"trusted":false},"cell_type":"code","source":"# EVAL\ntest_gen = test_datagen.flow(Xv, yv, batch_size=BATCH_SIZE, shuffle=False)\nprobabilities = model.predict_generator(test_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom support import print_confusion_matrix\n\ncnf_matrix = confusion_matrix(np.argmax(probabilities,axis=1), np.argmax(yv, axis=1))\n_ = print_confusion_matrix(cnf_matrix, selected_breed_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print Precision Recall F1-Score Report\nfrom sklearn.metrics import classification_report\n\nreport = classification_report(np.argmax(probabilities,axis=1), np.argmax(yv, axis=1), target_names=selected_breed_list)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## It's your turn\n\nTest out the model you just trained. Run the code Cell below and enter a URL with your favorite dog pic in the widget below. Have fun!ðŸŽ‰\n\n*Note*: By default, only the top 10 dog breeds available.\n\nHere are some URLs to test:\n\n- Bernese: https://cdn.pixabay.com/photo/2016/02/06/19/18/dog-1183475_960_720.jpg\n- Shih-tzu: https://cdn.pixabay.com/photo/2016/10/08/11/21/shih-tzu-puppy-1723492_960_720.jpg\n- Samoyed: https://cdn.pixabay.com/photo/2017/06/14/00/59/samoyed-2400687_960_720.jpg\n\nCan you do better? Play around with the model hyperparameters!"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Testing on url images\nfrom support import prediction_from_url\nfrom ipywidgets import interact_manual\nfrom ipywidgets import widgets\n\ndef get_prediction(URL):\n    prediction_from_url(URL, model, selected_breed_list)\n\ninteract_manual(get_prediction, URL=widgets.Text(placeholder='Insert URL with a dog pic'));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save the result"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Saving Model Weight\nmodel.save_weights('models/tl_xception_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### That's all folks - don't forget to shutdown your workspace once you're done ðŸ™‚"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}